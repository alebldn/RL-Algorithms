from __future__ import annotations 

import os 
import pickle
import shutil

from tqdm import tqdm

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
import gymnasium as gym

class A2C():
    """
    (Synchronous) Advantage Actor-Critic agent class

    Args: 
        n_features: The number of features of the input state
        n_actions: The number of actions the agent can take
        critic_lr: the learning rate for the critic network
        actor_lr: The learning rate for the actor network
        n_envs: the number of environment that run in parallel (on multiple CPU) to collect experiences
    """

    def __init__(self, state_shape: int, n_actions: int, critic_lr: float, actor_lr: float, n_envs: int) -> None:
        """Initializes the actor and critic networkks and their respective optimizers."""

        self.n_envs = n_envs
        self.n_actions = n_actions
        
        # define actor and critic layers
        inputs = tf.keras.layers.Input(shape=state_shape)
        actor_dense = tf.keras.layers.Dense(64, activation='elu') (inputs)
        actor = tf.keras.layers.Dense(n_actions, activation='softmax') (actor_dense)
        critic_dense = tf.keras.layers.Dense(64, activation='elu') (inputs)
        critic = tf.keras.layers.Dense(1, activation='linear') (critic_dense)

        self.model = tf.keras.Model(inputs=inputs, outputs=[actor, critic])

        self.actor_opt = tf.keras.optimizers.RMSprop(learning_rate=actor_lr)
        self.critic_opt = tf.keras.optimizers.RMSprop(learning_rate=critic_lr)


    def forward(self, x: np.ndarray) -> tuple[tf.Tensor, tf.Tensor]:
        """
        Forward pass of the networks.

        Args:
            x: A batched vector of states.

        Returns: 
            state_values: A tensor with the state values, with shape [n_envs,].
            action_probs: A tensor with the action probabilities, normalized with a softmax, with shape [n_envs, n_actions].
        """

        x = tf.convert_to_tensor(x)
        action_probs, state_values = self.model(x)
        return (action_probs, state_values)

    def select_action(self, x:np.ndarray) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:
        """
        Returns a tuple of the chosen actions and the log probs of those actions.

        Args:
            x: A batched vectors of states.

        Returns:
            actions: A tensor with the actions, with shape [n_envs]
            action_probs: A tensor with the probability of the actions with shape [n_envs]
            state_values: A tensor with the state values, with shape [n_envs]
            entropy: A tensor with the entropy values, with shape [n_envs]
        """

        action_probs, state_values = self.forward(x)
        action_pd = tfp.distributions.Categorical(probs=action_probs)
        actions = action_pd.sample()
        entropy = action_pd.entropy()
        return (actions, action_probs, state_values, entropy)

    def get_losses(self, states:tf.Tensor, actions: tf.Tensor, rewards: tf.Tensor, probs: tf.Tensor, value_preds: tf.Tensor, entropy: tf.Tensor, masks: tf.Tensor,
            gamma: float, lam: float, ent_coeff: float) -> tuple[tf.Tensor, tf.Tensor]:
        """
        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic using GAE to compute the advantages.
        
        Args:
            states: A tensor with the states previously explored, with shape [n_steps_per_update, n_envs]
            actions: A tensor with the actions taken, with shape [n_steps_per_update, n_envs]
            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].
            probs: A tensor with the probabilities of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].
            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].
            entropy: A tensor with the categorical entropy of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].
            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].
            gamma: The discount factor.
            lam: Lambda, the GAE hyperparamter. 
                (lambda = 1 corresponds to Monte-Carlo sampling with high variance and no bias,
                 lambda = 0 corresponds to normal TD-Learning that has a low variance but is biased because the estimates are generated by a Neural Net).

        Returns:
            critic_loss: The cirtic loss for the minibatch
            actor_loss: The actor loss for the minibatch
        """

        # action_log_probs = tf.math.log(tf.reduce_sum(tf.one_hot(actions, self.n_actions) * action_probs, axis=1))
        T = len(rewards)

        # compute the advantages using GAE - gae having shape: [n_steps_per_update, n_envs]
        gae = tf.zeros(self.n_envs, dtype=tf.float32)
        advantages = []
        advantages.append(gae)

        for t in reversed(range(T - 1)):
            td_error = (rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t])
            gae = td_error + gamma * lam * masks[t] * gae
            advantages.append(gae)

        advantages = tf.stack(advantages[::-1])

        # give a bonus for higher entropy to encourage exploration
        action_log_probs = tf.reduce_sum(tf.math.log(probs) * tf.one_hot(actions, self.n_actions), axis=-1)
        actor_loss = - tf.reduce_mean(tf.stop_gradient(advantages) * action_log_probs) - ent_coef * tf.reduce_mean(entropy)
        # actor_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) (y_true=actions, y_pred=probs, sample_weight=tf.stop_gradient(advantages)) - ent_coef * tf.reduce_mean(entropy) 
        
        # calculate the loss of the minibatch for actor ad critic
        critic_loss = tf.reduce_mean(advantages ** 2)

        return (critic_loss, actor_loss)

    def update_parameters(self, tape: tf.GradientTape, critic_loss: tf.Tensor, actor_loss: tf.Tensor) -> None:
        """
        Updates the parameters of the actor and critic networks.

        Args:
            tape: gradient tape used to compute gradient
            critic_loss: The critic loss.
            actor_loss: The actor loss.
        """

        actor_grads = tape.gradient(actor_loss, self.model.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        critic_grads = tape.gradient(critic_loss, self.model.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        self.actor_opt.apply_gradients(zip(actor_grads, self.model.trainable_variables))
        self.critic_opt.apply_gradients(zip(critic_grads, self.model.trainable_variables))


envs = gym.vector.make("LunarLander-v2", num_envs=3, max_episode_steps=600)

envs = gym.vector.AsyncVectorEnv(
    [
        lambda: gym.make("LunarLander-v2", gravity=-10.0, enable_wind=True, wind_power=15.0, turbulence_power=1.5, max_episode_steps=600),
        lambda: gym.make("LunarLander-v2", gravity=-9.8, enable_wind=True, wind_power=10.0, turbulence_power=1.3, max_episode_steps=600),
        lambda: gym.make("LunarLander-v2", gravity=-7.0, enable_wind=False, max_episode_steps=600),
    ]
)

# environment hyperparams
n_envs = 16
n_updates = 2000 
n_steps_per_update = 64
randomize_domain = True 

# agent hyperparams
gamma = 0.999
lam = 0.95
ent_coef = 0.01
actor_lr = 0.001
critic_lr = 0.005

if randomize_domain:
    envs = gym.vector.AsyncVectorEnv(
        [
            lambda: gym.make(
                "LunarLander-v2",
                gravity=np.clip(
                    np.random.normal(loc=-10.0, scale=1.0), a_min=-11.99, a_max=-0.01
                ),
                enable_wind=np.random.choice([True, False]),
                wind_power=np.clip(
                    np.random.normal(loc=15.0, scale=1.0), a_min=0.01, a_max=19.99
                ),
                turbulence_power=np.clip(
                    np.random.normal(loc=1.5, scale=0.5), a_min=0.01, a_max=1.99
                ),
                max_episode_steps=600,
            )
            for i in range(n_envs)
        ]
    )
else:
    envs = gym.vector.make("LunarLander-v2", num_envs=n_envs, max_episode_steps=600)

obs_shape = envs.single_observation_space.shape[0]
action_shape = envs.single_action_space.n

# init the agent
agent = A2C(obs_shape, action_shape, critic_lr, actor_lr, n_envs)

# create a wrapper environment to save episode returns and episode lengths
envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)

critic_losses = []
actor_losses = []
entropies = []

# use tqdm to get a progress bar for training
for sample_phase in tqdm(range(n_updates)):
    # we don't have to reset the envs, they just continue playing until the episode is over and then reset automatically
    # reset lists that collect experiences of an episode (sample phase)
    # minibatch variables

    # at the start of training, reset all envs to get an initial state
    if sample_phase == 0:
        states, info = envs_wrapper.reset(seed=42)

    with tf.GradientTape(persistent=True) as tape:
        # episode_variables
        ep_states = [] 
        ep_actions = [] 
        ep_value_preds = []
        ep_rewards = []
        ep_probs = []
        ep_masks = []
        ep_entropy = []

        # play  steps in our parallel environments to collect data
        for step in range(n_steps_per_update):
            # select an action A_{t} using S_{t} as input for the agent
            actions, probs, state_value_preds, entropy = agent.select_action(states)
        
            # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}
            states, rewards, terminated, truncated, infos = envs_wrapper.step(actions.numpy())
        
            ep_states.append(tf.convert_to_tensor(states))
            ep_actions.append(actions)
            ep_value_preds.append(tf.squeeze(state_value_preds)) 
            ep_rewards.append(tf.convert_to_tensor(rewards, dtype=tf.float32))
            ep_probs.append(probs)
            ep_entropy.append(entropy)
            ep_masks.append(tf.convert_to_tensor([not term for term in terminated], dtype=tf.float32))
        
        ep_states = tf.stack(ep_states)
        ep_actions = tf.stack(ep_actions)
        ep_value_preds = tf.stack(ep_value_preds)
        ep_rewards = tf.stack(ep_rewards)
        ep_probs = tf.stack(ep_probs)
        ep_entropy = tf.stack(ep_entropy)
        ep_masks = tf.stack(ep_masks)

        # calculate the losses for actor and critic
        critic_loss, actor_loss = agent.get_losses(ep_states, ep_actions, ep_rewards, ep_probs, ep_value_preds, ep_entropy, ep_masks, gamma, lam, ent_coef)

    # update the actor and critic networks
    agent.update_parameters(tape, critic_loss, actor_loss)
    
    # log the losses and entropy
    critic_losses.append(critic_loss.numpy())
    actor_losses.append(actor_loss.numpy())
    entropies.append(tf.reduce_mean(ep_entropy).numpy())

""" plot results """
# %matplotlib inline
rolling_length = 20
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))
fig.suptitle(f"Training plots for {agent.__class__.__name__} in the LunarLander-v2 environment\n \
        (n_envs = {n_envs}, n_steps_per_update = {n_steps_per_update}, randomize_domain = {randomize_domain})")

# episode return
axs[0][0].set_title("Episode Returns")
episode_returns_moving_average = (np.convolve(np.array(envs_wrapper.return_queue).flatten(), np.ones(rolling_length), mode="valid") / rolling_length)
axs[0][0].plot(np.arange(len(episode_returns_moving_average)) / n_envs, episode_returns_moving_average)
axs[0][0].set_xlabel("Number of episodes")

# entropy
axs[1][0].set_title("Entropy")
entropy_moving_average = (np.convolve(np.array(entropies), np.ones(rolling_length), mode="valid") / rolling_length)
axs[1][0].plot(entropy_moving_average)
axs[1][0].set_xlabel("Number of updates")

# critic_loss
axs[0][1].set_title("Critic Loss")
critic_losses_moving_average = (np.convolve(np.array(critic_losses).flatten(), np.ones(rolling_length), mode="valid") / rolling_length)
axs[0][1].plot(critic_losses_moving_average)
axs[0][1].set_xlabel("Number of updates")

# actor_loss
axs[1][1].set_title("Actor Loss")
actor_losses_moving_average = (np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode="valid") / rolling_length)
axs[1][1].plot(actor_losses_moving_average)
axs[1][1].set_xlabel("Number of updates")

plt.tight_layout()
plt.show()

print(ep_rewards)

play_env = gym.make(
                "LunarLander-v2",
                gravity=np.clip(
                    np.random.normal(loc=-10.0, scale=1.0), a_min=-11.99, a_max=-0.01
                ),
                enable_wind=np.random.choice([True, False]),
                wind_power=np.clip(
                    np.random.normal(loc=15.0, scale=1.0), a_min=0.01, a_max=19.99
                ),
                turbulence_power=np.clip(
                    np.random.normal(loc=1.5, scale=0.5), a_min=0.01, a_max=1.99
                ),
                max_episode_steps=600,
                render_mode='human',
            )


for _ in range(5):
    
    state, info = play_env.reset()
    while True:
        state = np.expand_dims(state, axis=0)
        action, _, _, _ = agent.select_action(state)
        action = action[0]
        state, reward, terminated, truncated, info = play_env.step(action.numpy())
        play_env.render()

        if terminated or truncated:
            break

play_env.close()
